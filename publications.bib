
@inproceedings{fang_kgpr_2023,
	address = {New York, NY, USA},
	series = {{CIKM} '23},
	title = {{KGPR}: {Knowledge} {Graph} {Enhanced} {Passage} {Ranking}},
	isbn = {9798400701245},
	shorttitle = {{KGPR}},
	url = {https://dl.acm.org/doi/10.1145/3583780.3615252},
	doi = {10.1145/3583780.3615252},
	abstract = {Passage ranking aims to rank a set of passages based on their relevance to a query. Current state-of-the-art models for this task typically employ a cross-encoder structure. However, these models lack access to background knowledge, i.e., information related to the query that can be helpful in retrieving relevant passages. Knowledge Graphs (KGs) provide a structured way of storing information about entities and their relationships, offering valuable background knowledge about entities. While KGs have been used to augment pretrained language models (LMs) to perform several reasoning tasks such as question answering, it remains an open question of how to utilise the information from KGs to enhance the performance of cross-encoders on the passage ranking task. Therefore, we propose KGPR, a KG-enhanced cross-encoder for the Passage Retrieval task. KGPR is built upon LUKE, an entity-aware pretrained LM, with an additional module that fuses information from KGs into LUKE. By leveraging the background knowledge from KGs, KGPR enhances the model's comprehension of queries and passages, resulting in improved ranking performance. Experimental results demonstrate that using KGs can enhance the performance of LUKE in the passage retrieval task, and KGPR can outperform state-of-the-art monoT5 cross-encoder by 3.32\% and 10.77\% on the MS MARCO development set and TREC DL-HARD query set respectively, using a model with a similar number of parameters.},
	urldate = {2023-11-11},
	booktitle = {Proceedings of the 32nd {ACM} {International} {Conference} on {Information} and {Knowledge} {Management}},
	publisher = {Association for Computing Machinery},
	author = {Fang, Jinyuan and Meng, Zaiqiao and Macdonald, Craig},
	month = oct,
	year = {2023},
	keywords = {cross-encoder, knowledge graphs, passage ranking},
	pages = {3880--3885},
	file = {Full Text PDF:files/3649/Fang et al. - 2023 - KGPR Knowledge Graph Enhanced Passage Ranking.pdf:application/pdf},
}

@inproceedings{wu_adaptive_2023,
	title = {Adaptive {Compositional} {Continual} {Meta}-{Learning}},
	url = {https://proceedings.mlr.press/v202/wu23d.html},
	abstract = {This paper focuses on continual meta-learning, where few-shot tasks are heterogeneous and sequentially available. Recent works use a mixture model for meta-knowledge to deal with the heterogeneity. However, these methods suffer from parameter inefficiency caused by two reasons: (1) the underlying assumption of mutual exclusiveness among mixture components hinders sharing meta-knowledge across heterogeneous tasks. (2) they only allow increasing mixture components and cannot adaptively filter out redundant components. In this paper, we propose an Adaptive Compositional Continual Meta-Learning (ACML) algorithm, which employs a compositional premise to associate a task with a subset of mixture components, allowing meta-knowledge sharing among heterogeneous tasks. Moreover, to adaptively adjust the number of mixture components, we propose a component sparsification method based on evidential theory to filter out redundant components. Experimental results show ACML outperforms strong baselines, showing the effectiveness of our compositional meta-knowledge, and confirming that ACML can adaptively learn meta-knowledge.},
	language = {en},
	urldate = {2023-11-11},
	booktitle = {Proceedings of the 40th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Wu, Bin and Fang, Jinyuan and Zeng, Xiangxiang and Liang, Shangsong and Zhang, Qiang},
	month = jul,
	year = {2023},
	note = {ISSN: 2640-3498},
	pages = {37358--37378},
	file = {Full Text PDF:files/3651/Wu et al. - 2023 - Adaptive Compositional Continual Meta-Learning.pdf:application/pdf},
}

@inproceedings{fang_manner_2023,
	address = {Toronto, Canada},
	title = {{MANNER}: {A} {Variational} {Memory}-{Augmented} {Model} for {Cross} {Domain} {Few}-{Shot} {Named} {Entity} {Recognition}},
	shorttitle = {{MANNER}},
	url = {https://aclanthology.org/2023.acl-long.234},
	doi = {10.18653/v1/2023.acl-long.234},
	abstract = {This paper focuses on the task of cross domain few-shot named entity recognition (NER), which aims to adapt the knowledge learned from source domain to recognize named entities in target domain with only a few labeled examples. To address this challenging task, we propose MANNER, a variational memory-augmented few-shot NER model. Specifically, MANNER uses a memory module to store information from the source domain and then retrieve relevant information from the memory to augment few-shot task in the target domain. In order to effectively utilize the information from memory, MANNER uses optimal transport to retrieve and process information from memory, which can explicitly adapt the retrieved information from source domain to target domain and improve the performance in the cross domain few-shot setting. We conduct experiments on English and Chinese cross domain few-shot NER datasets, and the experimental results demonstrate that MANNER can achieve superior performance.},
	urldate = {2023-11-11},
	booktitle = {Proceedings of the 61st {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Fang, Jinyuan and Wang, Xiaobin and Meng, Zaiqiao and Xie, Pengjun and Huang, Fei and Jiang, Yong},
	editor = {Rogers, Anna and Boyd-Graber, Jordan and Okazaki, Naoaki},
	month = jul,
	year = {2023},
	pages = {4261--4276},
	file = {Full Text PDF:files/3653/Fang et al. - 2023 - MANNER A Variational Memory-Augmented Model for C.pdf:application/pdf},
}

@article{wang_enhancing_2023,
	title = {Enhancing {Conversational} {Recommendation} {Systems} with {Representation} {Fusion}},
	volume = {17},
	issn = {1559-1131},
	url = {https://dl.acm.org/doi/10.1145/3577034},
	doi = {10.1145/3577034},
	abstract = {Conversational Recommendation Systems (CRSs) aim to improve recommendation performance by utilizing information from a conversation session. A CRS first constructs questions and then asks users for their feedback in each conversation session to refine better recommendation lists to users. The key design of CRS is to construct proper questions and obtain users’ feedback in response to these questions so as to effectively capture user preferences. Many CRS works have been proposed; however, they suffer from defects when constructing questions for users to answer: (1) employing a dialogue policy agent for constructing questions is one of the most common choices in CRS, but it needs to be trained with a huge corpus, and (2) it is not appropriate that constructing questions from a single policy (e.g., a CRS only selects attributes that the user has interacted with) for all users with different preferences. To address these defects, we propose a novel CRS model, namely a Representation Fusion–based Conversational Recommendation model, where the whole conversation session is divided into two subsessions (i.e., Local Question Search subsession and Global Question Search subsession) and two different question search methods are proposed to construct questions in the corresponding subsessions without employing policy agents. In particular, in the Local Question Search subsession we adopt a novel graph mining method to find questions, where the paths in the graph between users and attributes can eliminate irrelevant attributes; in the Global Question Search subsession we propose to initialize user preference on items with the user and all item historical rating records and construct questions based on user’s preference. Then, we update the embeddings independently over the two subsessions according to user’s feedback and fuse the final embeddings from the two subsessions for the recommendation. Experiments on three real-world recommendation datasets demonstrate that our proposed method outperforms five state-of-the-art baselines.},
	number = {1},
	urldate = {2023-11-11},
	journal = {ACM Transactions on the Web},
	author = {Wang, Yingxu and Chen, Xiaoru and Fang, Jinyuan and Meng, Zaiqiao and Liang, Shangsong},
	month = feb,
	year = {2023},
	keywords = {Conversational Recommendation Systems, interactive recommendation, representation learning},
	pages = {6:1--6:34},
	file = {Full Text PDF:files/3657/Wang et al. - 2023 - Enhancing Conversational Recommendation Systems wi.pdf:application/pdf},
}

@misc{cao_knowledge_2023,
	title = {Knowledge {Graph} {Embedding}: {A} {Survey} from the {Perspective} of {Representation} {Spaces}},
	shorttitle = {Knowledge {Graph} {Embedding}},
	url = {http://arxiv.org/abs/2211.03536},
	doi = {10.48550/arXiv.2211.03536},
	abstract = {Knowledge graph embedding (KGE) is an increasingly popular technique that aims to represent entities and relations of knowledge graphs into low-dimensional semantic spaces for a wide spectrum of applications such as link prediction, knowledge reasoning and knowledge completion. In this paper, we provide a systematic review of existing KGE techniques based on representation spaces. Particularly, we build a fine-grained classification to categorise the models based on three mathematical perspectives of the representation spaces: (1) Algebraic perspective, (2) Geometric perspective, and (3) Analytical perspective. We introduce the rigorous definitions of fundamental mathematical spaces before diving into KGE models and their mathematical properties. We further discuss different KGE methods over the three categories, as well as summarise how spatial advantages work over different embedding needs. By collating the experimental results from downstream tasks, we also explore the advantages of mathematical space in different scenarios and the reasons behind them. We further state some promising research directions from a representation space perspective, with which we hope to inspire researchers to design their KGE models as well as their related applications with more consideration of their mathematical space properties.},
	urldate = {2023-11-11},
	publisher = {arXiv},
	author = {Cao, Jiahang and Fang, Jinyuan and Meng, Zaiqiao and Liang, Shangsong},
	month = oct,
	year = {2023},
	note = {arXiv:2211.03536 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Information Retrieval, Computer Science - Machine Learning},
	annote = {Comment: 42 pages, 6 figures, 9 tables},
	file = {arXiv Fulltext PDF:files/3660/Cao et al. - 2023 - Knowledge Graph Embedding A Survey from the Persp.pdf:application/pdf;arXiv.org Snapshot:files/3661/2211.html:text/html},
}

@article{chen_multi-relational_2022,
	title = {Multi-{Relational} {Graph} {Representation} {Learning} with {Bayesian} {Gaussian} {Process} {Network}},
	volume = {36},
	copyright = {Copyright (c) 2022 Association for the Advancement of Artificial Intelligence},
	issn = {2374-3468},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/20492},
	doi = {10.1609/aaai.v36i5.20492},
	abstract = {Learning effective representations of entities and relations for knowledge graphs (KGs) is critical to the success of many multi-relational learning tasks. Existing methods based on graph neural networks learn a deterministic embedding function, which lacks sufficient flexibility to explore better choices when dealing with the imperfect and noisy KGs such as the scarce labeled nodes and noisy graph structure. To this end, we propose a novel multi-relational graph Gaussian Process network (GGPN), which aims to improve the flexibility of deterministic methods by simultaneously learning a family of embedding functions, i.e., a stochastic embedding function. Specifically, a Bayesian Gaussian Process (GP) is proposed to model the distribution of this stochastic function and the resulting representations are obtained by aggregating stochastic function values, i.e., messages, from neighboring entities. The two problems incurred when leveraging GP in GGPN are the proper choice of kernel function and the cubic computational complexity. To address the first problem, we further propose a novel kernel function that can explicitly take the diverse relations between each pair of entities into account and be adaptively learned in a data-driven way. We address the second problem by reformulating GP as a Bayesian linear model, resulting in a linear computational complexity. With these two solutions, our GGPN can be efficiently trained in an end-to-end manner. We evaluate our GGPN in link prediction and entity classification tasks, and the experimental results demonstrate the superiority of our method. Our code is available at https://github.com/sysu-gzchen/GGPN.},
	language = {en},
	number = {5},
	urldate = {2023-11-11},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Chen, Guanzheng and Fang, Jinyuan and Meng, Zaiqiao and Zhang, Qiang and Liang, Shangsong},
	month = jun,
	year = {2022},
	note = {Number: 5},
	keywords = {Machine Learning (ML)},
	pages = {5530--5538},
	file = {Full Text PDF:files/3663/Chen et al. - 2022 - Multi-Relational Graph Representation Learning wit.pdf:application/pdf},
}

@article{fang_hyperspherical_2021,
	title = {Hyperspherical {Variational} {Co}-embedding for {Attributed} {Networks}},
	volume = {40},
	issn = {1046-8188},
	url = {https://dl.acm.org/doi/10.1145/3478284},
	doi = {10.1145/3478284},
	abstract = {Network-based information has been widely explored and exploited in the information retrieval literature. Attributed networks, consisting of nodes, edges as well as attributes describing properties of nodes, are a basic type of network-based data, and are especially useful for many applications. Examples include user profiling in social networks and item recommendation in user-item purchase networks. Learning useful and expressive representations of entities in attributed networks can provide more effective building blocks to down-stream network-based tasks such as link prediction and attribute inference. Practically, input features of attributed networks are normalized as unit directional vectors. However, most network embedding techniques ignore the spherical nature of inputs and focus on learning representations in a Gaussian or Euclidean space, which, we hypothesize, might lead to less effective representations. To obtain more effective representations of attributed networks, we investigate the problem of mapping an attributed network with unit normalized directional features into a non-Gaussian and non-Euclidean space. Specifically, we propose a hyperspherical variational co-embedding for attributed networks (HCAN), which is based on generalized variational auto-encoders for heterogeneous data with multiple types of entities. HCAN jointly learns latent embeddings for both nodes and attributes in a unified hyperspherical space such that the affinities between nodes and attributes can be captured effectively. We argue that this is a crucial feature in many real-world applications of attributed networks. Previous Gaussian network embedding algorithms break the assumption of uninformative prior, which leads to unstable results and poor performance. In contrast, HCAN embeds nodes and attributes as von Mises-Fisher distributions, and allows one to capture the uncertainty of the inferred representations. Experimental results on eight datasets show that HCAN yields better performance in a number of applications compared with nine state-of-the-art baselines.},
	number = {3},
	urldate = {2023-11-11},
	journal = {ACM Transactions on Information Systems},
	author = {Fang, Jinyuan and Liang, Shangsong and Meng, Zaiqiao and De Rijke, Maarten},
	month = dec,
	year = {2021},
	keywords = {generalized variational auto-encoders, hyperspherical representation, Network embedding},
	pages = {58:1--58:36},
	file = {Full Text PDF:files/3667/Fang et al. - 2021 - Hyperspherical Variational Co-embedding for Attrib.pdf:application/pdf},
}

@inproceedings{zhang_variational_2021,
	title = {Variational {Continual} {Bayesian} {Meta}-{Learning}},
	volume = {34},
	url = {https://proceedings.neurips.cc/paper/2021/hash/cdd0500dc0ef6682fa6ec6d2e6b577c4-Abstract.html},
	abstract = {Conventional meta-learning considers a set of tasks from a stationary distribution. In contrast, this paper focuses on a more complex online setting, where tasks arrive sequentially and follow a non-stationary distribution. Accordingly, we propose a Variational Continual Bayesian Meta-Learning (VC-BML) algorithm. VC-BML maintains a Dynamic Gaussian Mixture Model for meta-parameters, with the number of component distributions determined by a Chinese Restaurant Process. Dynamic mixtures at the meta-parameter level increase the capability to adapt to diverse tasks due to a larger parameter space, alleviating the negative knowledge transfer problem. To infer posteriors of model parameters, compared to the previously used point estimation method, we develop a more robust posterior approximation method -- structured variational inference for the sake of avoiding forgetting knowledge. Experiments on tasks from non-stationary distributions show that VC-BML is superior in transferring knowledge among diverse tasks and alleviating catastrophic forgetting in an online setting.},
	urldate = {2023-11-11},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Zhang, Qiang and Fang, Jinyuan and Meng, Zaiqiao and Liang, Shangsong and Yilmaz, Emine},
	year = {2021},
	pages = {24556--24568},
	file = {Full Text PDF:files/3669/Zhang et al. - 2021 - Variational Continual Bayesian Meta-Learning.pdf:application/pdf},
}

@inproceedings{fang_structure-aware_2021,
	title = {Structure-{Aware} {Random} {Fourier} {Kernel} for {Graphs}},
	volume = {34},
	url = {https://proceedings.neurips.cc/paper/2021/hash/93da579a65ce84cd1d4c85c2cbb84fc5-Abstract.html},
	abstract = {Gaussian Processes (GPs) define distributions over functions and their generalization capabilities depend heavily on the choice of kernels. In this paper, we propose a novel structure-aware random Fourier (SRF) kernel for GPs that brings several benefits when modeling graph-structured data. First, SRF kernel is defined with a spectral distribution based on the Fourier duality given by the Bochner's theorem, transforming the kernel learning problem to a distribution inference problem. Second, SRF kernel admits a random Fourier feature formulation that makes the kernel scalable for optimization. Third, SRF kernel enables to leverage geometric structures by taking subgraphs as inputs. To effectively optimize GPs with SRF kernel, we develop a variational EM algorithm, which alternates between an inference procedure (E-step) and a learning procedure (M-step). Experimental results on five real-world datasets show that our model can achieve state-of-the-art performance in two typical graph learning tasks, i.e., object classification and link prediction.},
	urldate = {2023-11-11},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Fang, Jinyuan and Zhang, Qiang and Meng, Zaiqiao and Liang, Shangsong},
	year = {2021},
	pages = {17681--17694},
	file = {Full Text PDF:files/3671/Fang et al. - 2021 - Structure-Aware Random Fourier Kernel for Graphs.pdf:application/pdf},
}

@inproceedings{fang_gaussian_2021,
	address = {New York, NY, USA},
	series = {{KDD} '21},
	title = {Gaussian {Process} with {Graph} {Convolutional} {Kernel} for {Relational} {Learning}},
	isbn = {978-1-4503-8332-5},
	url = {https://dl.acm.org/doi/10.1145/3447548.3467327},
	doi = {10.1145/3447548.3467327},
	abstract = {Gaussian Process (GP) offers a principled non-parametric framework for learning stochastic functions. The generalization capability of GPs depends heavily on the kernel function, which implicitly imposes the smoothness assumptions of the data. However, common feature-based kernel functions are inefficient to model the relational data, where the smoothness assumptions implied by the kernels are violated. To model the complex and non-differentiable functions over relational data, we propose a novel Graph Convolutional Kernel, which enables to incorporate relational structures to feature-based kernels to capture the statistical structure of data. To validate the effectiveness of proposed kernel function in modeling relational data, we introduce GP models with Graph Convolutional Kernel in two relational learning settings, i.e., unsupervised settings of link prediction and semi-supervised settings of object classification. The parameters of our GP models are optimized through the scalable variational inducing point method. However, the highly structured likelihood objective requires densely sampling from variational distributions, which is costly and makes its optimization challenging in the unsupervised settings. To tackle this challenge, we propose a Local Neighbor Sampling technique with a provable more efficient computational complexity. Experimental results on real-world datasets demonstrate that our model achieves state-of-the-art performance in two relational learning tasks.},
	urldate = {2023-11-11},
	booktitle = {Proceedings of the 27th {ACM} {SIGKDD} {Conference} on {Knowledge} {Discovery} \& {Data} {Mining}},
	publisher = {Association for Computing Machinery},
	author = {Fang, Jinyuan and Liang, Shangsong and Meng, Zaiqiao and Zhang, Qiang},
	month = aug,
	year = {2021},
	keywords = {gaussian process, graph convolutional kernel, relational learning},
	pages = {353--363},
	file = {Full Text PDF:files/3675/Fang et al. - 2021 - Gaussian Process with Graph Convolutional Kernel f.pdf:application/pdf},
}

@inproceedings{meng_semi-supervisedly_2019,
	title = {Semi-supervisedly {Co}-embedding {Attributed} {Networks}},
	volume = {32},
	url = {https://proceedings.neurips.cc/paper_files/paper/2019/hash/0e7c7d6c41c76b9ee6445ae01cc0181d-Abstract.html},
	abstract = {Deep generative models (DGMs) have achieved remarkable advances. Semi-supervised variational auto-encoders (SVAE) as a classical DGM offers a principled framework to effective generalize from small labelled data to large unlabelled ones, but it is difficult to incorporate rich unstructured relationships within the multiple heterogeneous entities. In this paper, to deal with the problem, we present a semi-supervised co-embedding model for attributed networks (SCAN) based on the generalized SVAE for the heterogeneous data, which collaboratively learns low- dimensional vector representations of both nodes and attributes for partially labelled attributed networks semi-supervisedly. The node and attribute embeddings obtained in a unified manner by our SCAN can benefit not only for capturing the proximities between nodes but also the affinities between nodes and attributes. Moreover, our model also trains a discriminative network to learn the label predictive distribution of nodes. Experimental results on real-world networks demonstrate that our model yields excellent performance in a number of applications such as attribute inference, user profiling and node classification compared to the state-of-the-art baselines.},
	urldate = {2023-11-11},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Meng, Zaiqiao and Liang, Shangsong and Fang, Jinyuan and Xiao, Teng},
	year = {2019},
	file = {Full Text PDF:files/3677/Meng et al. - 2019 - Semi-supervisedly Co-embedding Attributed Networks.pdf:application/pdf},
}

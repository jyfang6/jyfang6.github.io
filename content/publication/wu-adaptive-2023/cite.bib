@inproceedings{wu_adaptive_2023,
 abstract = {This paper focuses on continual meta-learning, where few-shot tasks are heterogeneous and sequentially available. Recent works use a mixture model for meta-knowledge to deal with the heterogeneity. However, these methods suffer from parameter inefficiency caused by two reasons: (1) the underlying assumption of mutual exclusiveness among mixture components hinders sharing meta-knowledge across heterogeneous tasks. (2) they only allow increasing mixture components and cannot adaptively filter out redundant components. In this paper, we propose an Adaptive Compositional Continual Meta-Learning (ACML) algorithm, which employs a compositional premise to associate a task with a subset of mixture components, allowing meta-knowledge sharing among heterogeneous tasks. Moreover, to adaptively adjust the number of mixture components, we propose a component sparsification method based on evidential theory to filter out redundant components. Experimental results show ACML outperforms strong baselines, showing the effectiveness of our compositional meta-knowledge, and confirming that ACML can adaptively learn meta-knowledge.},
 author = {Wu, Bin and Fang, Jinyuan and Zeng, Xiangxiang and Liang, Shangsong and Zhang, Qiang},
 booktitle = {Proceedings of the 40th International Conference on Machine Learning},
 file = {Full Text PDF:files/3651/Wu et al. - 2023 - Adaptive Compositional Continual Meta-Learning.pdf:application/pdf},
 language = {en},
 month = {July},
 note = {ISSN: 2640-3498},
 pages = {37358--37378},
 publisher = {PMLR},
 title = {Adaptive Compositional Continual Meta-Learning},
 url = {https://proceedings.mlr.press/v202/wu23d.html},
 urldate = {2023-11-11},
 year = {2023}
}

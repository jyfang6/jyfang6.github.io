@inproceedings{zhang_variational_2021,
 abstract = {Conventional meta-learning considers a set of tasks from a stationary distribution. In contrast, this paper focuses on a more complex online setting, where tasks arrive sequentially and follow a non-stationary distribution. Accordingly, we propose a Variational Continual Bayesian Meta-Learning (VC-BML) algorithm. VC-BML maintains a Dynamic Gaussian Mixture Model for meta-parameters, with the number of component distributions determined by a Chinese Restaurant Process. Dynamic mixtures at the meta-parameter level increase the capability to adapt to diverse tasks due to a larger parameter space, alleviating the negative knowledge transfer problem. To infer posteriors of model parameters, compared to the previously used point estimation method, we develop a more robust posterior approximation method -- structured variational inference for the sake of avoiding forgetting knowledge. Experiments on tasks from non-stationary distributions show that VC-BML is superior in transferring knowledge among diverse tasks and alleviating catastrophic forgetting in an online setting.},
 author = {Zhang, Qiang and Fang, Jinyuan and Meng, Zaiqiao and Liang, Shangsong and Yilmaz, Emine},
 booktitle = {Advances in Neural Information Processing Systems},
 file = {Full Text PDF:files/3669/Zhang et al. - 2021 - Variational Continual Bayesian Meta-Learning.pdf:application/pdf},
 pages = {24556--24568},
 publisher = {Curran Associates, Inc.},
 title = {Variational Continual Bayesian Meta-Learning},
 url = {https://proceedings.neurips.cc/paper/2021/hash/cdd0500dc0ef6682fa6ec6d2e6b577c4-Abstract.html},
 urldate = {2023-11-11},
 volume = {34},
 year = {2021}
}
